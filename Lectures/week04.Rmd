---
title: "R for Health Data Science"
subtitle: "Week 04: Biostatistics Part 1"
author: "Sam Stewart"
date: "2021-03-12"
output: 
  pdf_document:
    number_sections: true
  # html_document:
  #   number_sections: true
  #   toc: true
  #   toc_depth: 3
  #   toc_float:
  #     collapsed: false
---
  
```{r setup, include=FALSE}

library(Hmisc)
library(kableExtra)
library(dplyr)
library(tidyr)


knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.width=8, fig.height=5)
#This option should force knitr to use the project workng directory
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

options(full_width = FALSE)
options(knitr.kable.NA = '')

```

```{r readFram}

dat = read.csv("data/framinghamFirst.csv",header=TRUE, 
               na.strings=".",stringsAsFactors=FALSE)
dat$BMIGroups = cut(dat$BMI,breaks=c(0,18.5,25,30,Inf),
                    labels=c("Underweight","Normal","Overweight","Obese"))
dat$SEX = factor(dat$SEX,levels=1:2,labels=c("Male","Female"))
dat$DIABETES = factor(dat$DIABETES,levels=0:1,labels=c("No Diabetes","Diabetes"))
dat$HYPERTEN = factor(dat$HYPERTEN,levels=0:1,labels=c("Normotensive","Hypertensive"))

```
# Introduction
This week we're going to learn some actual statistics - largely the material covered in Biostats I in a typical graduate epi program.  We'll cover the following functions

* Hypothesis tests
  - t-tests
  - chi-square tests
  - Fisher's tests
* regression models
  - linear regression
  - ANOVA and post-hoc comparisons
  - logistic regression
  - general linear models (GLM)

# Formulas
We've seen formulas once before with boxplots, but they're worth covering again since they'll be used in several functions today.  The general structure of a formula is `y~x`, where we're predicting `y` with `x`, or splitting `y` by `x`.  Formulas are evaluated from the tilde-out, so in places where order matters they will evaluate in a specific order.  Consider the following simple boxplot example.

```{r boxplotFormulaExample}

boxplot(SYSBP~SEX+DIABETES,data=dat,col=2:3)
boxplot(SYSBP~DIABETES+SEX,data=dat,col=2:3)

```

The order of the boxes is dependent on the structure of the formula. This won't matter in most uses (testing, modeling) but when it does be aware that you can re-arrange the formulas to alter the presentation of the results.

## T-tests
We'll use the same command for 1-sample, 2-sample independent and 2-sample paired t-tests, `t.test`.  

|Format|Use|
|--|---------------------------|
|`t.test(x,mu=0)`|is a one-sample t-test of the variable `x` against a null value of 0.|
|`t.test(x,y)`|is an independent 2-sample t-test comparing the values of `x` to the values of `y`|
|`t.test(x,y,paired=TRUE)`|can be used for a paired t-test, though you could also create a variable called `z=y-x` and then do a 1-sample test on `z`|
|`t.test(y~x)`|is a 2-sample independent t-test of the variable `y` across the levels of `x`|

While the main purpose of the function is to perform p-value testing, the most valuable part is the CIs and effect estimates, so we'll look at how to extract values from the function.

```{r t.test.examples01}

#testing Systolic blood pressure against a null of 0 
t.test(dat$SYSBP)
#testing against a null of 130
t.test(dat$SYSBP,mu=130)
#saving the value and extracting the components
tTest01 = t.test(dat$SYSBP,mu=130)
tTest01
names(tTest01)
tTest01$statistic
tTest01$p.value
tTest01$estimate
tTest01$conf.int


```
You can see how you can extract all the components you might need - we'll see later in reporting how you can use these extractions to produce effective, concise reports.

```{r t.test.examples02}
#testing systolic against diastolic BP
t.test(dat$SYSBP,dat$DIABP)
#testing systolic between sexes
t.test(dat$SYSBP~dat$SEX)
#almost all functions that take equations will let you submit a dataset
t.test(SYSBP~SEX,data=dat)

#we can extract the BPs using the tapply function
SBP.sex = tapply(dat$SYSBP,dat$SEX,c)
t.test(SBP.sex$Male,SBP.sex$Female)
```
### Aside: `tapply()`
This is the first time we've seen an apply function (I think), they'll become incredibly useful later for organizing your analyses.

In this example we used tapply:
```
tapply(X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE)
```

`tapply()` is used when we want to split a vector (`X`) by the levels of another vector (`INDEX`), and apply a function (`FUN`) to each of the split values.  In this example we used the function `c()` or the concatenate function, as we just wanted to get all the SYSBP values, stratified by SEX.


```{r tapply.ex}

#get the average SYSBP for each value of SEX
tapply(dat$SYSBP,dat$SEX,mean)
#get the average for each BMI group
tapply(dat$SYSBP,dat$BMIGroups,mean)
#the functions don't need to be numeric
par(mfrow=c(2,2))
temp = tapply(dat$SYSBP,dat$BMIGroups,hist)#this doesn't work well for titles

```

We'll see other apply functions later in the course: `lapply, sapply, apply` are all valuable in certain situations.

## Chi-square tests
For testing categorical data we use the function `chisq.test()`, or `fisher.test()` if you need the non-parametric version.  The typical use case will be to create a 2x2 (or RxC) table using the command `table()`, and then perform the test on the table.  The functions will take the vectors themselves, but you tend to want the table anyway.

```{r chisq01}

tab01 = table(dat$SEX,dat$HYPERTEN)
chi01 = chisq.test(tab01,correct=FALSE)#I think this matches STATA
chi01 = chisq.test(tab01)#with the Yate's correction
chi01
names(chi01)
chi01$statistic
chi01$p.value

#this also works, but no table
chisq.test(dat$SEX,dat$HYPERTEN)

```

Unlike with `t.test()` there's no effect measure here - the chi-square test doesn't calculate one, so you'll have to get the OR/RR/RD yourself.  Fisher's test is nice in that regard - it produces an OR, even though it doesn't use it in the calculation.

```{r fisher01}
fisher.test(tab01)
fish01 = fisher.test(tab01)
names(fish01)
fish01$estimate
fish01$conf.int
```

### Getting Effect Measures from Tables
Getting RR/OR/RD isn't in base-R - you can build a function yourself, or you can use the `epiR` library.

```{r epiR}

library(epiR)
test01 = epi.2by2(tab01)
test01
names(test01)
test01$res$RR.crude.wald

```

I find the function a bit overkill, but it definitely can do everything you need (see the help file for more details, `?epi.2by2`).  

The one contingency is that it assumes the table has a very specific arrangement - it assumes that the rows are exposed +/-, and the columns are outcomes +/-. While this is a logical arrangement, it is usually the opposite order in R, which tends to sort the rows/columns low-to-high, which means they're usually -/+.  You should make sure to get your table in the correct order before you submit it.

```{r epiR02}

tab01#wrong arrangement of columns
tab01 = tab01[,2:1]#flip the columns
tab01
epi.2by2(tab01)

```

# Regression
The two main functions we'll look at for regression are `lm` for linear regression and `glm` for general linear regression, which includes logistic.  There are a couple of other functions that we'll need, but those two are the core.

## Linear Regression
```{r lm01}

mod.lm01 = lm(SYSBP~DIABP,data=dat)
mod.lm01
summary(mod.lm01)

```

This builds a linear regression model predicting systolic BP with diastolic BP.  The model returns just the coefficients, while the `summary()` returns a detailed description, including coefficient testing.

Both the `lm` object and the `summary` object have components that we might want to access:

```{r lm02}

names(mod.lm01)
mod.lm01$coeff
sum = summary(mod.lm01)
sum$coefficients
sum$r.squared
sum$fstatistic

```

You can extract almost everything you need from either the model or the summary object - the coefficient table from the summary object is the most useful.

To check our assumptions (which I'm sure you all do religiously and never forget about) we just plot the object - it produces 4 plots, so we'll need to change the plotting space.

```{r lm.plot01,fig.height=8,fig.width=8}

par(mfrow=c(2,2))
plot(mod.lm01)

```

I only tend to like the first two plots, luckily theres a `which` option to control which plots are produced.

```{r lm.plot02, fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(mod.lm01,which=c(1,2))
```

## Multiple Linear Regression
```{r lm.multi01}

library(car)
mod.lm02 = lm(SYSBP~DIABP+AGE+SEX+BMIGroups+DIABETES+CURSMOKE,data=dat)
car::Anova(mod.lm02)
summary(mod.lm02)

```

For multiple regression the formula connects the predictors with `+`.  We can get summaries the same way, but if we want to perform ANOVA tests on the individual factors we use the `Anova()` function from the `car` library.

**Don't use the `anova()` command from the `base` library - it performs sequential sum of squares, which no-one ever wants.**  There are many in the R community that consider this a bug, but it's been this way for too long, and R values backward compatibility, so it will never be fixed.  I always preface my Anova analyses with the `car::` library specification to make sure I never make a mistake.

```{r lm.multi02}

confint(mod.lm02)

```

The command `conf.int` will produce the confidence intervals on the estimates - you could pull the table yourself from `summary(mod.lm02)$coefficients` and calculate the CI using the values there, but this function is more efficient and less prone to errors.

## Post-hoc comparisons
I won't be teaching anything about the anova analysis itself - there is a command called `aov()` to perform Anova's, but as everyone here knows an Anova is just a linear regression, so I would stick to the `lm()` command.

The one place that `aov()` is needed is post-hoc analyses - the functions for Tukey and Bonferroni post-hoc analyses require an `aov()` object, so we'll have to convert.

```{r postHoc01}

mod.lm03 = lm(SYSBP~BMIGroups,data=dat)
#first convert to aov
aov03 = aov(mod.lm03)
#TukeyHSD performs the post-hoc CIs using the Tukey Honest Significant Difference
TukeyHSD(aov03)

```

There is a more complete library called `multcomp` if you need to complex post-hoc comparisons - I find the syntax confusing, but if you have complex linear-combinations of coefficients for testing then this library can help.

```{r multcomp01}

library(multcomp)
CIs = glht(mod.lm03,linfct=mcp(BMIGroups='Tukey'))
summary(CIs,test=adjusted(type='bonferroni'))

```
## Logistic Regressions
Logistic regression will work the same way, the only difference is that we're using a generalized function, so we need to specify that we're working with a binary outcome.  To do this we specify the `family=binomial` argument.  **Don't forget this argument** - the default is to perform a linear regression, so if you pass a 1/0 variable and don't set the `family` value you get a result that looks correct but is very wrong.

```{r logistic01}

mod.log01 = glm(CURSMOKE~GLUCOSE,data=dat,family=binomial)
#THIS IS THE WRONG MODEL SPEC
mod.log01.wrong = glm(CURSMOKE~GLUCOSE,data=dat)
summary(mod.log01)
summary(mod.log01.wrong)

```

There's nothing in the output or the model results to clue you into the model being wrong. If your outcome is a factor then it will break, another good argument for using factors.  When predicting a factor variable it will take the first level as the control and the second level as the event.

```{r logistic02}

mod.log02 = glm(DIABETES~GLUCOSE,data=dat,family=binomial)
#This will throw an error since DIABETES is a factor variable
#mod.log02 = glm(DIABETES~GLUCOSE,data=dat)
mod.log02
summary(mod.log02)

```

R doesn't naturally produce Odds Ratios, so you'll have to do it yourself.

```{r logsitic.OR01}

mod.log03 = glm(CURSMOKE~SEX+SYSBP+GLUCOSE+BMIGroups,data=dat,family=binomial)
#extract the coefficients
coef = mod.log03$coefficients
#get the CIs on the coefficients
ci = confint(mod.log03)
#tie them together in a table
out = cbind(OR=coef,ci)
#exponentiate them to get the ORs
out01 = exp(out)

#or in one line
out02 = exp(cbind(OR=mod.log03$coefficients,confint(mod.log03)))

```

This is a place where self-written functions can be useful - I've written a command called `modelOR()` that takes a model and produces the confidence interval itself.  A brief version called `getOR()` is given below:

```{r logistic.OR02}

getOR = function(mod){
  out = exp(cbind(OR=mod$coefficients,confint(mod)))
  return(out)
}
out03 = getOR(mod.log03)

```

Home made functions are useful if you're running the same code over and over - in the `function()` you specify the arguments you need, and then between the braces you operate on those arguments.  As the full `modelOR()` function below shows at the end of this code, they can get rather complex.  Feel free to keep that `modelOR()` function for your own use, I've been using it for years and I *think* all the kinks are worked out.

# Breakout Exercise
We covered 5 major testing commands today (along with several other helper commands)

* t-test
* chisq.test
* fisher.test
* lm
* glm

Using those commands test the following hypotheses (in whatever manner you see fit):

1. Is there a difference in glucose levels between smokers and non-smokers?
2. Is the difference still present after controlling for sex?
3. Is there an interaction between smoking and sex?
4. Is there an effect of smoking on diabetes status?
5. Is the effect of smoking on diabetes present after controlling for sex?

# `modelOR()` Code
```{r modelOR.code}

modelOR = function(model,alpha=0.05,pvalue=FALSE){
  z = qnorm(1-alpha/2,0,1)
  lev = model$xlevels
  lab = names(model$model)[-1]
  dat = model$data
  lreg.coeffs <- coef(summary(model))
  k=2
  rowNames = matrix(rep(vector(length=dim(lreg.coeffs)[1]),4),ncol=4)
  rowNames[1,] = c('(Intercept)','','','')
  for(i in 1:length(lab)){
    if(lab[i]%in%names(lev)){
      # categorical variable
      for(j in 2:length(lev[[(1:length(lev))[names(lev)==lab[i]]]])){
        rowNames[k,] = c(paste(lab[i],":"),lev[[(1:length(lev))[names(lev)==lab[i]]]][j],'vs.',lev[[(1:length(lev))[names(lev)==lab[i]]]][1])
        k=k+1        
      }
    }
    else{
      rowNames[k,] = c(paste(lab[i],':'),'1','unit','increase')
      k=k+1      
    }
  }
  lci <- exp(lreg.coeffs[ ,1] - z * lreg.coeffs[ ,2])
  or <- exp(lreg.coeffs[ ,1])
  uci <- exp(lreg.coeffs[ ,1] + z * lreg.coeffs[ ,2])
  lreg.or <- data.frame(cbind(lci, or, uci))
  orNames = vector(length=dim(rowNames)[1])
  for(i in 1:dim(lreg.or)[1]){
    if(lreg.or[i,2] < 1){
      # invert the odds ratio
      tempLci = 1/lreg.or[i,3]
      tempOR = 1/lreg.or[i,2]
      tempUci = 1/lreg.or[i,1]
      lreg.or[i,] = c(tempLci,tempOR,tempUci)
      # fix the row names
      if(rowNames[i,3]=='vs.'){
        temp = rowNames[i,2]
        rowNames[i,2] = rowNames[i,4]
        rowNames[i,4] = temp
      }
      else{
        rowNames[i,4] = 'decrease'
      }
    }
    orNames[i] = paste(rowNames[i,1],rowNames[i,2],rowNames[i,3],rowNames[i,4])
  }
  names(lreg.or) = c("Lower CI","OR","Upper CI")
  if(pvalue){
    lreg.or[,4] = (summary(model))$coefficients[,4]
    names(lreg.or)[4] = "p-value"
  }
  #The function doesn't handle interaction models well, even though I usually only want to first order ORs, so I need to fix the rowNames
  if(sum(duplicated(orNames))>0){
    orNames[which(duplicated(orNames))] = LETTERS[1:length(which(duplicated(orNames)))]
  }
  row.names(lreg.or) = orNames
  if(pvalue){
    
  }
  return(lreg.or)
}

```